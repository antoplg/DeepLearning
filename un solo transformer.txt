# Define the encodings

def positional_encoding(length, depth):
  depth = depth/2

  positions = np.arange(length)[:, np.newaxis]    
  depths = np.arange(depth)[np.newaxis, :]/depth   
      
  angle_rates = 1 / (10000**depths)         
  angle_rads = positions * angle_rates     

  pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], 
                                axis=-1) 

  return tf.cast(pos_encoding, dtype=tf.float32)


class PositionalEmbedding(tf.keras.layers.Layer):
  def __init__(self, vocab_size, d_model):
    super().__init__()
    self.d_model = d_model
    self.embedding = tf.keras.layers.Embedding(vocab_size,
                                               d_model, 
                                               mask_zero=True) 
    
    self.pos_encoding = positional_encoding(length=2048, 
                                            depth=d_model)


  def compute_mask(self, *args, **kwargs):
    return self.embedding.compute_mask(*args, **kwargs)

  def call(self, x):
    length = tf.shape(x)[1]
    x = self.embedding(x)
    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))                
    x = x + self.pos_encoding[tf.newaxis, :length, :]
    return x


class ClassicalEmbedding(tf.keras.layers.Layer):
  def __init__(self, vocab_size, d_model):
    super().__init__()
    self.d_model = d_model
    self.embedding = tf.keras.layers.Embedding(vocab_size,
                                               d_model, 
                                               mask_zero=True) 


  def call(self, x):
    x = self.embedding(x)
    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))              

    return x
  

# define the Attention 
class Attention(Layer):
  def __init__(self, num_heads, key_dim, dropout_rate):
        super().__init__()
        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)
        self.normalization = LayerNormalization(epsilon=1e-6)
        self.dropout = Dropout(dropout_rate)

  def call(self, query, value, key, casual_mask=False, training=True):
        attn_output = self.mha(query=query, value=value, key=key, use_causal_mask=casual_mask, training=training)
        attn_output = self.dropout(attn_output, training=training)
        out = self.normalization(query + attn_output)
        return out
        

# Define the Feed Forword
class FeedForward(Layer):
  def __init__(self, d_model, dff, dropout_rate):
        super().__init__()
        self.first_dense = Dense(dff, activation='relu')
        self.second_dense = Dense(d_model, activation='relu')
        self.norm = LayerNormalization(epsilon=1e-6)
        self.dropout = Dropout(dropout_rate)

  def call(self, x, training=True):
        fd = self.first_dense(x)
        sd = self.second_dense(fd)
        output = self.dropout(sd, training=training)
        output = self.norm(x + output)
        return output


# Define the Encoder layer
class EncoderLayer(Layer):
    
    def __init__(self, d_model, num_heads, dff, dropout_rate):
        super().__init__()
        self.self_attetion = Attention(num_heads=num_heads, key_dim=d_model, dropout_rate=dropout_rate)
        self.feed_forward = FeedForward(d_model, dff, dropout_rate)
    
    def call(self, x, training=True):
        attn_output = self.self_attetion(query=x, value=x, key=x, training=training)
        ffn_output = self.feed_forward(attn_output)
        return ffn_output
  

# Define the Encoder
class Encoder(Layer):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, dropout_rate, embedding_type):
        super().__init__()
        self.d_model = d_model
        self.num_layers = num_layers

        if embedding_type == 'positional':
          self.embedding = PositionalEmbedding(input_vocab_size, d_model)
        else:
           self.embedding =  ClassicalEmbedding(input_vocab_size, d_model)

        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]
        self.dropout = Dropout(dropout_rate)

    def call(self, x, training=True):
        x = self.embedding(x)
        x = self.dropout(x, training=training)

        for i in range(self.num_layers):
            x = self.enc_layers[i](x, training=training)

        return x


# Define the Decoder layer
class DecoderLayer(Layer):
    
    def __init__(self, d_model, num_heads, dff, dropout_rate):
        super(DecoderLayer, self).__init__()
        self.self_attetion = Attention(num_heads=num_heads, key_dim=d_model, dropout_rate=dropout_rate)
        self.mha = Attention(num_heads=num_heads, key_dim=d_model, dropout_rate=dropout_rate)
        self.feed_forward = FeedForward(d_model, dff, dropout_rate)


    def call(self, x, enc_output, training=True):
        self_attention = self.self_attetion(query=x, value=x, key=x, casual_mask=True, training=training)
        multi_head_attention = self.mha(query=self_attention, value=enc_output, key=enc_output, training=training)
        ffn_output = self.feed_forward(multi_head_attention)

        return ffn_output
    

# Define the Decoder
class Decoder(Layer):
    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, dropout_rate):
        super(Decoder, self).__init__()
        self.d_model = d_model
        self.num_layers = num_layers

        self.pos_encoding = PositionalEmbedding(target_vocab_size, d_model)
        self.dropout = Dropout(dropout_rate)

        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]

    def call(self, x, enc_output, training=True):

        x = self.pos_encoding(x)
        x = self.dropout(x, training=training)
        for i in range(self.num_layers):
            x = self.dec_layers[i](x, enc_output, training=training)

        return x

class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
  def __init__(self, d_model, warmup_steps=4000):
    super().__init__()

    self.d_model = d_model
    self.d_model = tf.cast(self.d_model, tf.float32)

    self.warmup_steps = warmup_steps
    

  def __call__(self, step):
    step = tf.cast(step, dtype=tf.float32)
    arg1 = tf.math.rsqrt(step)
    arg2 = step * (self.warmup_steps ** -1.5)

    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

def masked_loss(label, pred):
  mask = label != 0
  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=False, reduction='none')
  loss = loss_object(label, pred)

  mask = tf.cast(mask, dtype=loss.dtype)
  loss *= mask

  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)
  return loss


def masked_accuracy(label, pred):
  pred = tf.argmax(pred, axis=2)
  label = tf.cast(label, pred.dtype)
  match = label == pred

  mask = label != 0

  match = match & mask

  match = tf.cast(match, dtype=tf.float32)
  mask = tf.cast(mask, dtype=tf.float32)
  return tf.reduce_sum(match)/tf.reduce_sum(mask)

# Model Parameter
transformer_params = {
    "num_layers" : 4,
    "d_model" : 128,
    "num_heads" : 7,
    "dff" : 254,
    "input_vocab_size" : 10000,
    "output_vocab_size" : 10000,
    "dropout_rate" : 0.15,
    "encoder_type" : 'not-positional'
}


class Transformer(Model):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, output_vocab_size, dropout_rate, encoder_type):
        super(Transformer, self).__init__()
        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, dropout_rate, encoder_type)
        self.decoder = Decoder(num_layers, d_model, num_heads, dff, output_vocab_size, dropout_rate)
        self.final_layer = Dense(output_vocab_size, activation='softmax')                                                    

    def call(self, input, training=True):
        context, input_sentence = input
        encoder_output = self.encoder(context, training=training)
        decoder_output = self.decoder(input_sentence, encoder_output, training=training)
        final_output = self.final_layer(decoder_output)
        try:
          del final_output._keras_mask
        except AttributeError:
          pass

        return final_output
    
    def compile(self, lr):
        learning_rate = CustomSchedule(lr)

        optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)

        self.compile(loss=masked_loss,
                    optimizer=optimizer,
                    metrics=[masked_accuracy])

    def save(self, path):
        self.save_weights(path)

    def load(self, path):
        self.load_weights(path)

    # A customized training method that makes the two transformers train separately  
    def fit(self, x, y, validation_split = .1, epochs=6, batch_size=256, callbacks_1=[], callbacks_2=[]):
        x1, x2 = x
        data_size = x1.shape[0]
        print('-'*200)
        print("training transformer 1:")
        print('-' * 200)
        history = self.fit([x1[:data_size//2], 
                                          x2[:data_size//2]], 
                                          y[:data_size//2],
                                          epochs=epochs, 
                                          batch_size=batch_size, 
                                          validation_split=validation_split, 
                                          callbacks=callbacks_1
                                          )
        print('-'*200)
        print("training transformer 2:")
        print('-' * 200)
        return history



model = Transformer(**transformer_params)
model([y_train[:1], x_train[:1]])
model.compile(128)
model.summary()
